https://www.datacamp.com/community/tutorials/apache-spark-python
https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python
https://github.com/jupyter/docker-demo-images/blob/master/notebooks/Welcome%20to%20Spark%20with%20Python.ipynb
https://www.kaggle.com/fatmakursun/pyspark-ml-tutorial-for-beginners
https://nbviewer.jupyter.org/github/gabrielspmoreira/static_resources/blob/gh-pages/Kaggle-Outbrain-PageViews_EventsAnalytics.ipynb
https://www.kaggle.com/acmiyaguchi/pyspark-dataframe-preprocessing-for-cord-19

1. Spark Performance: Scala or Python?
	a. In general, most developers seem to agree that Scala wins in terms of performance and concurrency: it’s definitely faster than Python when you’re working with Spark, and when you’re talking about concurrency
	b. Note that asynchronous code allows for non-blocking I/O when making calls to remote services. Let’s state it differently with an example: when you have two lines of code of which the first one queries a database and the next prints something to the console, synchronous programming will wait for the query to finish before printing something. Your program is (momentarily) blocked. If your programming language doesn’t support asynchronous programming, you’ll need to make threads to execute lines of code in parallel. Asynchronous programming, on the other hand, will already print to the console while the database is being queried. The query will be processed on the background.

	c. In short, the above explains why it’s still strongly recommended to use Scala over Python when you’re working with streaming data, even though structured streaming in Spark seems to reduce the gap already.

	d. But streaming data is not the only performance consideration that you might make.

	e. 	When you’re working with the DataFrame API, there isn’t really much of a difference between Python and Scala, but you do need to be wary of User Defined Functions (UDFs), which are less efficient than its Scala equivalents. That’s why you should favor built-in expressions if you’re working with Python. When you’re working with Python, also make sure not to pass your data between DataFrame and RDD unnecessarily, as the serialization and deserialization of the data transfer is particularly expensive.
	
2. Starting to use PySpark
	a. You can immediately start working in the Spark shell by typing ./bin/pyspark. SparkContext has already been initialized. You don’t need to import SparkContext from pyspark to begin working.
	b. ./bin/pyspark --master local[*]
		In the following command, you see that the --master argument allows you to specify to which master the SparkContext connects to. In this case, you see that the local mode is activated. The number in between the brackets designates the number of cores that are being used; In this case, you use all cores, while local[4] would only make use of four cores.
		Note that the application UI is available at localhost:4040. Open up a browser, paste in this location and you’ll get to see a dashboard with tabs designating jobs, stages, storage, etc. This will definitely come in handy when you’re executing jobs and looking to tune them. You’ll read more about this further on.
		
3. Running Spark Applications Using Jupyter Notebooks
	a. Now that you have all that you need to get started, you can launch the Jupyter Notebook Application by typing the following:

		PYSPARK_DRIVER_PYTHON="jupyter" PYSPARK_DRIVER_PYTHON_OPTS="notebook" pyspark
		Or you can launch Jupyter Notebook normally with jupyter notebook and run the following code before importing PySpark:

		! pip install findspark 
		With findspark, you can add pyspark to sys.path at runtime. Next, you can just import pyspark just like any other regular library:

		import findspark
		findspark.init()

		# Or the following command
		findspark.init("/path/to/spark_home")

		from pyspark import SparkContext, SparkConf
		
		Note that if you haven’t installed Spark with brew and in accordance with the instructions that are listed above, it could be that you need to add the path to SPARK_HOME to findspark.init(). 		   If you’re still in doubt where SPARK_HOME is located at, you can call findspark.find() to automatically detect the location of where Spark is installed.
		
	b. sudo docker pull jupyter/pyspark-notebook
		sudo docker run --rm -it -p 8890:8888 -v "$(pwd):/home/jovyan/work" jupyter/pyspark-notebook
4. Spark APIs: RDD, Dataset and DataFrame : PySpark seems to only have RDD and Dataframe as Datasets require statically typed variables whereas python is dynamically typed.
	a. RDDs: Resilient Distributed Datasets
		RDDs are the building blocks of Spark. It’s the original API that Spark exposed and pretty much all the higher level APIs decompose to RDDs. From a developer’s perspective, an RDD is simply 		  a set of Java or Scala objects representing data.

		RDDs have three main characteristics: they are compile-time type safe (they have a type!), they are lazy and they are based on the Scala collections API.

		The advantages of RDDs are manifold, but there are also some problems. For example, it’s easy to build inefficient transformation chains, they are slow with non-JVM languages such as 				Python, they can not be optimized by Spark. Lastly, it’s difficult to understand what is going on when you’re working with them, because, for example, the transformation chains are not very 		  readable in the sense that you don’t immediately see what will be the solution, but how you are doing it.

	b. DataFrames
		Because of the disadvantages that you can experience while working with RDDs, the DataFrame API was conceived: it provides you with a higher level abstraction that allows you to use a query 		  language to manipulate the data. This higher level abstraction is a logical plan that represents data and a schema. This means that the frontend to interacting with your data is a lot 			easier! Because the logical plan will be converted to a physical plan for execution, you’re actually a lot closer to what you’re doing when you’re working with them rather than how you’re 		trying to do it, because you let Spark figure out the most efficient way to do what you want to do.

		Remember though that DataFrames are still built on top of RDDs!

		And exactly because you let Spark worry about the most efficient way to do things, DataFrames are optimized: more intelligent decisions will be made when you’re transforming data and that 		also explains why they are faster than RDDs.

		More specifically, the performance improvements are due to two things, which you’ll often come across when you’re reading up DataFrames: custom memory management (project Tungsten), which 		will make sure that your Spark jobs much faster given CPU constraints, and optimized execution plans (Catalyst optimizer), of which the logical plan of the DataFrame is a part.
	
	c. DataSets
		The only downside to using DataFrames is that you’ve lost compile-time type safety when you work with DataFrames, which makes your code more prone to errors. This is part of the reason why		they have moved more to the notion of Datasets: getting back some type safety and the use of lambda functions, which means that you want to go a bit back to the advantage that RDDs has to 		offer, but you don’t want to lose all the optimalizations that the DataFrames offer.
		The notion of the Dataset has developed and has become the second main Spark API, besides the RDD API. As a result, the Dataset can take on two distinct characteristics: a strongly-typed 			API and an untyped API. This means that the DataFrame is still there conceptually, as a synonym for a Dataset: any DataFrame is now a synonym for Dataset[Row] in Scala, where Row is a 			generic untyped JVM object. The Dataset is a collection of strongly-typed JVM objects.

		Note that, since Python has no compile-time type-safety, only the untyped DataFrame API is available. Or, in other words, Spark DataSets are statically typed, while Python is a dynamically 		 typed programming language. That explains why the DataFrames or the untyped API is available when you want to work with Spark in Python. Also, remember that Datasets are built on top of 			RDDs, just like DataFrames.

		To summarize, the clear advantage of working with the DataSet API (which includes both DataSets and DataFrames) are the static typing and the runtime type safety, the higher level 				abstraction over the data, and the performance and optimization. Of course, it also helps that the DataSet API basically forces you to work with more structured data, which also adds to the 		  ease of use of the API itself.
		
5. RDD Actions versus Transformations
	RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on 		the dataset. For example, map() is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce() is an action 	 that aggregates all the elements of the RDD using some function and returns the final result to the driver program. Note, however, that there is also a reduceByKey() that returns a distributed 	  dataset.

	All transformations in Spark are lazy, in that they do not compute their results right away: instead, they just remember the transformations applied to some base dataset. The transformations 		are only computed when an action requires a result to be returned to the driver program.

6. Instead of creating a copy of the variable for each machine, you use broadcast variables to send some immutable state once to each worker. Broadcast variables allow the programmer to keep a cached read-only variable in every machine. In short, you use these variables when you want a local copy of a variable.

You can create a broadcast variable with SparkContext.broadcast(variable). This will return the reference of the broadcast variable.

As you can see, persisting an RDD or using a broadcast variable are two different solutions to different problems.

7. What Are The Best Practices in Spark?
	There are tons of possibilities when you’re working with PySpark, but that doesn’t mean that there are some simple and general best practices that you can follow:

	Use Spark DataFrames
	Consider the section above to see whether you should use RDDs or DataFrames. As you already read above, Spark DataFrames are optimized and therefore also faster than RDDs. Especially when you’re working with structured data, you should really consider switching your RDD to a DataFrame.

	RDD Best Practices
	Don’t call collect() on large RDDs
	By calling collect() on any RDD, you drag data back into your applications from the nodes. Each RDD element will be copy onto the single driver program, which will run out of memory and crash. Given the fact that you want to make use of Spark in the most efficient way possible, it’s not a good idea to call collect() on large RDDs.

	Other functions that you can use to inspect your data are take() or takeSample(), but also countByKey(), countByValue() or collectAsMap() can help you out. If you really need to take a look at the complete data, you can always write out the RDD to files or export it to a database that is large enough to keep your data.

	Reduce Your RDD Before Joining
	The fact that you can chain operations comes in handy when you’re working with Spark RDDs, but what you might not realize is that you have a responsibility to build efficient transformation chains, too. Taking care of the efficiency is also a way of tuning your Spark jobs’ efficiency and performance.

	One of the most basic rules that you can apply when you’re revising the chain of operations that you have written down is to make sure that you filter or reduce your data before joining it. This way, you avoid sending too much data over the network that you’ll throw away after the join, which is already a good reason, right?

	But there is more. The join operation is one of the most expensive operations that you can use in Spark, so that’s why it makes sense to be wary of this. When you reduce the data before the join, you avoid shuffling your data around too much.

	Avoid groupByKey() on large RDDs
	On big data sets, you’re better off making use of other functions, such as reduceByKey(), combineByKey() or foldByKey(). When you use groupByKey(), all key-value pairs are shuffled around in the cluster. A lot of unnecessary data is being transferred over the network. Additionally, this also means that if more data is shuffled onto a single machine than can fit in memory, the data will be spilled to disk. This heavily impacts the performance of your Spark job.

	When you make use of reduceByKey(), for example, the pairs with the same key are already combined before the data is shuffled. As a result, you’ll have to send less data over the network. Next, the reduce function is called again so that all the values from each partition are reduced.

	Broadcast Variables
	Since you already know what broadcast variables are and in which situations they can come in handy, you’ll also have gathered that this is one of the best practices for when you’re working with Spark because you can reduce the cost of launching a job over the cluster.

	Avoid flatmap(), join() and groupBy() Pattern
	When you have two datasets that are grouped by key and you want to join them, but still keep them grouped, use cogroup() instead of the above pattern.