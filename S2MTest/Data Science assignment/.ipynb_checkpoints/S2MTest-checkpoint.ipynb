{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas worked upon: \n",
    "\n",
    "\n",
    "1. Length of story : Longer the story, harder to understand\n",
    "2. Length of sentence : Longer the sentence, harder to understand\n",
    "3. Length Of Words : Longer the Words, harder to understand\n",
    "4. Frequency of words : Lesser the frequency, more the unfamiliarity of the words\n",
    "\n",
    "Based on these data points we can work on the different indexes for readability like Gunning fog index or Flesch Kincaid indexes or create our own indexes based in ranges to decide what can be the difficulty for the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_colwidth = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob   \n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location to the path where files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Story text files/*.txt'   \n",
    "files=glob.glob(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Story text files/541.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[359]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for fileToRead in files:\n",
    "    with open(fileToRead, 'r') as file:\n",
    "        data.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDf = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf['fileName'] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf.columns = ['Text', 'fileName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = re.compile('[0-9]|-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf.loc[:,'Text'] = dataDf.Text.apply(lambda x: regexp.sub('',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf.loc[:, 'StoryLength'] = dataDf.Text.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some files were empty, thus removing those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf.drop(dataDf[dataDf.StoryLength == 0].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(dataDf.StoryLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>fileName</th>\n",
       "      <th>StoryLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>BOW!MEOW!WOW!</td>\n",
       "      <td>./Story text files/541.txt</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Text                    fileName  StoryLength\n",
       "359  BOW!MEOW!WOW!  ./Story text files/541.txt           13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDf[dataDf.StoryLength==13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [Four, friends, want, race, toys.Veen, green, car., Meena, yellow, auto.Sanju, red, bus., Manju, blue, lorry.Ready, ,, GO!, comes, Lucky., Haha,, Lucky, joined, race.Look,, look!, Lucky, fastest, all., Lucky, wins, race.]\n",
       "1                 [Rabbit, sleeping, apple, tree., apple, fell, branch.A, voice, said,, \"Run, Rabbit, run!, \", woke, quickly, ran, away, great, speed.She, met, Chicken., \"Why, running?, \", asked, Chicken., Rabbit, replied,, \"I, know., heard, something, falling, voice, said,, 'Run, Rabbit, run!, '\"Chicken, frightened., started, running, heard, Rabbit, said.They, met, Dog., Dog, asked,, \"Why, running?, \", Chicken, said,, \"I, know., heard, Rabbit, said,, know., heard, something, falling, voice, said,, 'Run, Rabbit, run!, '\"Dog, surprised, heard., started, running, Rabbit, Chicken.They, met, Horse., Horse, asked, Dog,, \"Why, running?, \", Dog, said,, \"I, know., heard, Chicken, said,, know., heard, Rabbit, said, know., heard, something, falling, voice, said,, ...]\n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [go, school, every, day?It, good, you.Why, bath, every, day?It, good, you.Why, play, sun?It, good, you.Why, stay, late?It, good, you.Why, sleep, every, day?Your, body, needs, rest.Why, listen, you?, know, why.]\n",
       "3       [swamp, far, side, Far, Forest,, lived, Sniffles,, Crocodile., Sniffles, always, crying, big, crocodile, tears, going, sniff., ., ., sniff., ., ., Even, mom, quite, tired, would, say,, “Stop, snivelling, crying,, start, behaving, like, crocodile., Go, hunt, food.”But, Sniffles, would, burst, tears, thought, eating, another, animal., mom, dad, feed, Sniffles, every, day.Sniffles, want, crocodile., 'I, ugly,, ', thought,, bursting, tears., even, want, live, like, crocodiles, do., best, friend, Punch,, golden, yellow, butterfly., Punch, wanted, big, strong., Punch, always, feeler, curled, fist, saying,, “Look, out!, punch, mouth.”They, made, funny, pair., Sniffles, swimming, Punch, perched, head., crocodiles, would, laugh, make, fun, them., bother, ...]\n",
       "4    [long, time, ago,, village, Pakshipur,, situated, edge, forest,, lived, young, boy, called, Baku., loved, counting, things.After, school,, helped, mother, weigh, vegetables, count, money., One, evening,, Baku, saw, pair, colourful, birds., laughed, took, off., Baku, followed, them.They, flew, cave, forest., Everything, different,, like, birds., blue, monkeys, whistling?, pink, elephants?, seemed, humming!, Baku, didn’t, see, sun, go, down., Soon,, got, dark,, lost.A, short, distance, away,, saw, roaring, fire., woman, pacing, down., name, Midu., Baku, went, asked, softly,, “Wha., ., ., matter?”“My, tenyearold, daughter, Koya, came, rare, illness, six, months, ago., hasn’t, able, sit, since, then!”, “Is, cure?”, asked, Baku., “Only, one—the, nectar, ...]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDf.Text.head().apply(lambda x: [j for j in x.split() if j.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Length Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf['wordLength'] = dataDf.Text.apply(lambda x: [len(j) for j in re.split('\\.|\\ |\\!|\\?', x) if j.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the longest length word in the story\n",
    "dataDf['maxWordLength'] = dataDf.wordLength.apply(lambda x: max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the average word length in the story\n",
    "dataDf['avgWordLength'] = dataDf.wordLength.apply(lambda x: mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking words with length smaller than 8 i.e. we are segregating smaller and longer words as longer \n",
    "# words might be difficult to read\n",
    "dataDf['smallWords'] = dataDf.wordLength.apply(lambda x: sum(np.array(x) < 8))  \n",
    "dataDf['largeWords'] = dataDf.wordLength.apply(lambda x: sum(np.array(x) >= 8))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Length Character Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf['sentenceLengthChar'] = dataDf.Text.apply(lambda x: [len(j) for j in re.split(\"!|\\.|\\?\", x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDf['maxSentenceLengthChar'] = dataDf.sentenceLengthChar.apply(lambda x: max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf['avgSentenceLengthChar'] = dataDf.sentenceLengthChar.apply(lambda x: mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking sentences with length smaller than 50 chars i.e. we are segregating smaller and longer sentences as longer \n",
    "# sentences might be difficult to read\n",
    "dataDf['smallSentenceChar'] = dataDf.sentenceLengthChar.apply(lambda x: sum(np.array(x) < 50))  \n",
    "dataDf['largeSentenceChar'] = dataDf.sentenceLengthChar.apply(lambda x: sum(np.array(x) >= 50))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Length Word Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf['sentenceLengthWord'] = dataDf.Text.apply(lambda x: [len(re.findall(\" \", j)) + 1 for j in re.split(\"!|\\.|\\?\", x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDf['maxSentenceLengthWord'] = dataDf.sentenceLengthWord.apply(lambda x: max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf['avgSentenceLengthWord'] = dataDf.sentenceLengthWord.apply(lambda x: mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking sentences with length smaller than 9 words i.e. we are segregating smaller and longer sentences as longer \n",
    "# sentences might be difficult to read\n",
    "dataDf['smallSentenceWord'] = dataDf.sentenceLengthWord.apply(lambda x: sum(np.array(x) < 9))  \n",
    "dataDf['largeSentenceWord'] = dataDf.sentenceLengthWord.apply(lambda x: sum(np.array(x) >= 9))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding rare words in the text documets can help us in deciding which words might be less familiar or occur less number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFreq = {}\n",
    "for row in dataDf.Text.iteritems():\n",
    "    for word in set(re.split(r\"\\.|\\!|\\?|\\ |\\'|\\\\|\\\"|\\,\", row[1])):\n",
    "        lemmatizedWord = lemmatizer.lemmatize(word)\n",
    "        if lemmatizedWord.lower():\n",
    "            if lemmatizedWord not in wordFreq:\n",
    "                wordFreq[lemmatizedWord] = 1\n",
    "            else:\n",
    "                wordFreq[lemmatizedWord] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('being').lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFreq['being']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Meena', 1),\n",
       " ('Veen', 1),\n",
       " ('lorry', 1),\n",
       " ('Lucky', 1),\n",
       " ('starte', 1),\n",
       " ('Horse', 1),\n",
       " ('grandaunt', 1),\n",
       " ('“Punch', 1),\n",
       " ('gnashing', 1),\n",
       " ('snivelling', 1),\n",
       " ('behaving', 1),\n",
       " ('Sniffles', 1),\n",
       " ('Sniffles’', 1),\n",
       " ('sniffling', 1),\n",
       " ('punch', 1),\n",
       " ('men’s', 1),\n",
       " ('acted', 1),\n",
       " ('“Men', 1),\n",
       " ('Punch', 1),\n",
       " ('whipping', 1),\n",
       " ('feeler', 1),\n",
       " ('menacingly', 1),\n",
       " ('next—can', 1),\n",
       " ('numbers—let’s', 1),\n",
       " ('days*', 1),\n",
       " ('pacing', 1),\n",
       " ('“Wha', 1),\n",
       " ('cured', 1),\n",
       " ('bloomed', 1),\n",
       " ('minimum', 1),\n",
       " ('factors**', 1),\n",
       " ('Koya', 1),\n",
       " ('LCM', 1),\n",
       " ('b—is', 1),\n",
       " ('Mathematics', 1),\n",
       " ('Midu', 1),\n",
       " ('factorization', 1),\n",
       " ('hmmm', 1),\n",
       " ('lunar', 1),\n",
       " ('Pakshipur', 1),\n",
       " ('”Baku’s', 1),\n",
       " ('Midu’s', 1),\n",
       " ('one—the', 1),\n",
       " ('tenyearold', 1),\n",
       " ('cycle—that', 1),\n",
       " ('Baku', 1),\n",
       " ('vadapav', 1),\n",
       " ('(LCM)', 1),\n",
       " ('“Baku', 1),\n",
       " ('Lowest', 1),\n",
       " ('average', 1),\n",
       " ('**If', 1),\n",
       " ('pavs', 1),\n",
       " ('Multiple', 1),\n",
       " ('(Hint:', 1),\n",
       " ('coincide', 1),\n",
       " ('Neelumbera', 1),\n",
       " ('Jauna', 1),\n",
       " ('Joseph', 1),\n",
       " ('yarn', 1),\n",
       " ('Geetha', 1),\n",
       " ('shift', 1),\n",
       " ('sews', 1),\n",
       " ('Grace', 1),\n",
       " ('Soapy', 1),\n",
       " ('autowalla', 1),\n",
       " ('scarf', 1),\n",
       " ('Noncooperation', 1),\n",
       " ('cluck', 1),\n",
       " ('tuber', 1),\n",
       " ('Ramuramu', 1),\n",
       " ('butted', 1),\n",
       " ('Alariwo', 1),\n",
       " ('Gbam', 1),\n",
       " ('Salewa', 1),\n",
       " ('Meeeeh', 1),\n",
       " ('wrinkling', 1),\n",
       " ('Ehen', 1),\n",
       " ('rhinoceros', 1),\n",
       " ('Rhinoceros', 1),\n",
       " ('“HELLO', 1),\n",
       " ('RHINOCEROS', 1),\n",
       " ('Hemu', 1),\n",
       " ('Rhin', 1),\n",
       " ('wallow', 1),\n",
       " ('necked', 1),\n",
       " ('Ring…', 1),\n",
       " ('Neelu', 1),\n",
       " ('Nelu', 1),\n",
       " ('Rhino…Rhino…”', 1),\n",
       " ('eagle…', 1),\n",
       " ('knobby', 1),\n",
       " ('Rhino…', 1),\n",
       " ('Rhinos…', 1),\n",
       " ('honk', 1),\n",
       " ('“Rhino…', 1),\n",
       " ('Rhin…Hehehe…”', 1),\n",
       " ('brownishgrey', 1),\n",
       " ('bobbed', 1),\n",
       " ('”“This', 1),\n",
       " ('RHIN', 1),\n",
       " ('Gajaraj', 1),\n",
       " ('Harini', 1),\n",
       " ('“Rhin', 1),\n",
       " ('Ringo…', 1),\n",
       " ('Ringo', 1),\n",
       " ('shy…', 1),\n",
       " ('Natkhat', 1),\n",
       " ('Kaziranga', 1),\n",
       " ('matted', 1),\n",
       " ('Bison', 1),\n",
       " ('Kalia', 1),\n",
       " ('RINGO…', 1),\n",
       " ('RINGO', 1),\n",
       " ('RHIN…Rhin…”', 1),\n",
       " ('RHINOCEROS…', 1),\n",
       " ('he…Rhin…Rhin', 1),\n",
       " ('gravel', 1),\n",
       " ('BIGBIG', 1),\n",
       " ('Meeep', 1),\n",
       " ('wheelywheely', 1),\n",
       " ('Wheels', 1),\n",
       " ('millstone', 1),\n",
       " ('shushed', 1),\n",
       " ('onlywonly', 1),\n",
       " ('merry', 1),\n",
       " ('dabba', 1),\n",
       " ('wheelies', 1),\n",
       " ('wheelies\\x00', 1),\n",
       " ('tring', 1),\n",
       " ('ghungroos', 1),\n",
       " ('soundmaking', 1),\n",
       " ('Shazia', 1),\n",
       " ('ghunghroos', 1),\n",
       " ('fiftyninth', 1),\n",
       " ('hushed', 1),\n",
       " ('“Maisha', 1),\n",
       " ('moonfish', 1),\n",
       " ('MEET', 1),\n",
       " ('lightproducing', 1),\n",
       " ('”Maisha', 1),\n",
       " ('confuse', 1),\n",
       " ('planktonic', 1),\n",
       " ('photophores', 1),\n",
       " ('plankter', 1),\n",
       " ('Maisha', 1),\n",
       " ('“Uchli', 1),\n",
       " ('“Lead', 1),\n",
       " ('trigger', 1),\n",
       " ('Uchli', 1),\n",
       " ('neon', 1),\n",
       " ('squid’s', 1),\n",
       " ('anglerfish', 1),\n",
       " ('LIGHTMAKERS', 1),\n",
       " ('right—their', 1),\n",
       " ('“Hurrah', 1),\n",
       " ('impress', 1),\n",
       " ('Man’s', 1),\n",
       " ('akka’s', 1),\n",
       " ('HABBA', 1),\n",
       " ('whateveritis', 1),\n",
       " ('Sujju', 1),\n",
       " ('scrabble', 1),\n",
       " ('tuttuts', 1),\n",
       " ('AIYO', 1),\n",
       " ('Dinku', 1),\n",
       " ('Chikkamma’s', 1),\n",
       " ('ojist’):', 1),\n",
       " ('meteorology', 1),\n",
       " ('‘meetyourall', 1),\n",
       " ('wellknown', 1),\n",
       " ('remainder', 1),\n",
       " ('eightandahalf', 1),\n",
       " ('Mukkaram', 1),\n",
       " ('directive', 1),\n",
       " ('Eighteen', 1),\n",
       " ('“Read', 1),\n",
       " ('sons’', 1),\n",
       " ('indulge', 1),\n",
       " ('Iran', 1),\n",
       " ('Remembering', 1),\n",
       " ('quarrelling', 1),\n",
       " ('decisions;', 1),\n",
       " ('Turan', 1),\n",
       " ('Abdullah’s', 1),\n",
       " ('oneninth', 1),\n",
       " ('“With', 1),\n",
       " ('vasiyat', 1),\n",
       " ('‘advice’', 1),\n",
       " ('Abdullah', 1),\n",
       " ('Mukarram', 1),\n",
       " ('naseehat', 1),\n",
       " ('property', 1),\n",
       " ('ninth', 1),\n",
       " ('Allah', 1),\n",
       " ('fulfilled', 1),\n",
       " ('pedigreed', 1),\n",
       " ('Saad', 1),\n",
       " ('granary', 1),\n",
       " ('Rukaiya', 1),\n",
       " ('Muhammad', 1),\n",
       " ('Arabia', 1),\n",
       " ('inform', 1),\n",
       " ('Khatoon', 1),\n",
       " ('Ahmad', 1),\n",
       " ('”Druvi', 1),\n",
       " ('Banyan', 1),\n",
       " ('Dragonflies', 1),\n",
       " ('starshaped', 1),\n",
       " ('Druvi', 1),\n",
       " ('TouchMeNot', 1),\n",
       " ('Maldives', 1),\n",
       " ('Druvi’s', 1),\n",
       " ('Wandering', 1),\n",
       " ('Tanzania', 1),\n",
       " ('Jungle', 1),\n",
       " ('sprayed', 1),\n",
       " ('Shobhini’s', 1),\n",
       " ('tonga', 1),\n",
       " ('pony', 1),\n",
       " ('Shobhini', 1),\n",
       " ('zoomed', 1),\n",
       " ('”Rain', 1),\n",
       " ('Pillu', 1),\n",
       " ('Hair', 1),\n",
       " ('goldfish', 1),\n",
       " ('‘Wow', 1),\n",
       " ('Combs', 1),\n",
       " ('tooted', 1),\n",
       " ('toolkits', 1),\n",
       " ('gushing', 1),\n",
       " ('“CRACK', 1),\n",
       " ('“Anything', 1),\n",
       " ('flooding', 1),\n",
       " ('wrench', 1),\n",
       " ('toolkit', 1),\n",
       " ('GETTING', 1),\n",
       " ('Yontnik', 1),\n",
       " ('unclean', 1),\n",
       " ('Yontra', 1),\n",
       " ('chime', 1),\n",
       " ('LAUGHING', 1),\n",
       " ('Dhruva', 1),\n",
       " ('happier', 1),\n",
       " ('“AND', 1),\n",
       " ('Gangtok', 1),\n",
       " ('Razia', 1),\n",
       " ('sundial', 1),\n",
       " ('dirtier', 1),\n",
       " ('BOINKKKKK', 1),\n",
       " ('SPITTING', 1),\n",
       " ('“Eeks', 1),\n",
       " ('spitting', 1),\n",
       " ('bunched', 1),\n",
       " ('hover', 1),\n",
       " ('Vidyalaya', 1),\n",
       " ('span', 1),\n",
       " ('STOP', 1),\n",
       " ('“Ohhh', 1),\n",
       " ('Louder', 1),\n",
       " ('Defecating', 1),\n",
       " ('HEADACHE', 1),\n",
       " ('excursion', 1),\n",
       " ('”“Aman', 1),\n",
       " ('contaminated', 1),\n",
       " ('EEEE', 1),\n",
       " ('tingling', 1),\n",
       " ('dirtometer', 1),\n",
       " ('Yantra', 1),\n",
       " ('flickered', 1),\n",
       " ('Pole', 1),\n",
       " ('tinker', 1),\n",
       " ('Shuddh', 1),\n",
       " ('Samrat', 1),\n",
       " ('Splatters', 1),\n",
       " ('Observatory', 1),\n",
       " ('”Aman', 1),\n",
       " ('Mantar', 1),\n",
       " ('spic', 1),\n",
       " ('pained', 1),\n",
       " ('nondegradable', 1),\n",
       " ('EYES', 1),\n",
       " ('inky', 1),\n",
       " ('breeding', 1),\n",
       " ('meter', 1),\n",
       " ('umm', 1),\n",
       " ('yantra', 1),\n",
       " ('allergic', 1),\n",
       " ('Tuberculosis', 1),\n",
       " ('Tonk', 1),\n",
       " ('Spit', 1),\n",
       " ('disgraceful', 1),\n",
       " ('LITTERING', 1),\n",
       " ('defecate', 1),\n",
       " ('breakage', 1),\n",
       " ('Sewage', 1),\n",
       " ('“Oohhh', 1),\n",
       " ('pavement', 1),\n",
       " ('Yontrik', 1),\n",
       " ('Gurung', 1),\n",
       " ('Pema', 1),\n",
       " ('diarrhoea', 1),\n",
       " ('”“There', 1),\n",
       " ('gutka', 1),\n",
       " ('Chakra', 1),\n",
       " ('hefty', 1),\n",
       " ('Yontriks', 1),\n",
       " ('splattered', 1),\n",
       " ('contaminating', 1),\n",
       " ('phlegm', 1),\n",
       " ('“Razia', 1),\n",
       " ('“Aman', 1),\n",
       " ('“Bhaiya', 1),\n",
       " ('Frrrrrrrrrr', 1),\n",
       " ('Throwing', 1),\n",
       " ('unusable', 1),\n",
       " ('BOINKs', 1),\n",
       " ('bhelpuri', 1),\n",
       " ('jaundice', 1),\n",
       " ('NOSE', 1),\n",
       " ('Splashing', 1),\n",
       " ('typhoid', 1),\n",
       " ('spittle', 1),\n",
       " ('“Pema', 1),\n",
       " ('urinating', 1),\n",
       " ('BOINK', 1),\n",
       " ('Diseases', 1),\n",
       " ('Jantar', 1),\n",
       " ('spit', 1),\n",
       " ('”Pema', 1),\n",
       " ('digital', 1),\n",
       " ('flaring', 1),\n",
       " ('Sawai', 1),\n",
       " ('urinate', 1),\n",
       " ('gazed', 1),\n",
       " ('Payal', 1),\n",
       " ('underneath', 1),\n",
       " ('“Maaamaaa', 1),\n",
       " ('teddy', 1),\n",
       " ('”“Let’s', 1),\n",
       " ('tomÁs', 1),\n",
       " ('ilyas', 1),\n",
       " ('Pirate', 1),\n",
       " ('Daaaddyyy', 1),\n",
       " ('”Samira', 1),\n",
       " ('Samira', 1),\n",
       " ('“Samira', 1),\n",
       " ('Ugh', 1),\n",
       " ('ugh', 1),\n",
       " ('minute…”', 1),\n",
       " ('them—idlis', 1),\n",
       " ('Shutting', 1),\n",
       " ('“Worms', 1),\n",
       " ('”Bret', 1),\n",
       " ('Spincy', 1),\n",
       " ('‘famous’;', 1),\n",
       " ('nearer', 1),\n",
       " ('nursery', 1),\n",
       " ('hawthorn', 1),\n",
       " ('“Incy', 1),\n",
       " ('Peel', 1),\n",
       " ('wiser', 1),\n",
       " ('Bit’s', 1),\n",
       " ('bandaged', 1),\n",
       " ('wincy', 1),\n",
       " ('friends;', 1),\n",
       " ('harm’s', 1),\n",
       " ('kinder', 1),\n",
       " ('“Peel', 1),\n",
       " ('Between', 1),\n",
       " ('Spincy’s', 1),\n",
       " ('”Spincy', 1),\n",
       " ('Bit', 1),\n",
       " ('‘famous’', 1),\n",
       " ('“Spincy', 1),\n",
       " ('Bret’s', 1),\n",
       " ('Bret', 1),\n",
       " ('passwords)', 1),\n",
       " ('Passwords', 1),\n",
       " ('‘unlock’', 1),\n",
       " ('password”', 1),\n",
       " ('nosey', 1),\n",
       " ('abcd', 1),\n",
       " ('Password', 1),\n",
       " ('Phone', 1),\n",
       " ('reverse', 1),\n",
       " ('biometric', 1),\n",
       " ('“if', 1),\n",
       " ('copying', 1),\n",
       " ('retina', 1),\n",
       " ('key’s', 1),\n",
       " ('keypad', 1),\n",
       " ('Riju', 1),\n",
       " ('“Hoi', 1),\n",
       " ('don’ts:', 1),\n",
       " ('Fingerprints', 1),\n",
       " ('pet’s', 1),\n",
       " ('pestering', 1),\n",
       " ('rasping', 1),\n",
       " ('“Hmmm…', 1),\n",
       " ('almirah’s', 1),\n",
       " ('(ComPUteR)', 1),\n",
       " ('unlock', 1),\n",
       " ('xyz', 1),\n",
       " ('town)', 1),\n",
       " ('Ranjith', 1),\n",
       " ('locksmith', 1),\n",
       " ('Riju’s', 1),\n",
       " ('”“Really', 1),\n",
       " ('cousin’s', 1),\n",
       " ('Ranjith’s', 1),\n",
       " ('“Incorrect', 1),\n",
       " ('tuna', 1),\n",
       " ('pawp', 1),\n",
       " ('SoFiJellyfish', 1),\n",
       " ('SoFi', 1),\n",
       " ('seafloor', 1),\n",
       " ('smuggler', 1),\n",
       " ('Eel', 1),\n",
       " ('shhhhrrrrrrrSoFi', 1),\n",
       " ('RoboTuna', 1),\n",
       " ('OctoBot', 1),\n",
       " ('Jellyfish', 1),\n",
       " ('paawwwwwp', 1),\n",
       " ('shhhhrrrrrrr', 1),\n",
       " ('earthwormLight', 1),\n",
       " ('grassUp', 1),\n",
       " ('dragonflyQuick', 1),\n",
       " ('snailBirds', 1),\n",
       " ('sunsetAbove', 1),\n",
       " ('rockOver', 1),\n",
       " ('Cherry', 1),\n",
       " ('tiktiktiktik', 1),\n",
       " ('smartTo', 1),\n",
       " ('Flowerpeckers', 1),\n",
       " ('Flowerpecker', 1),\n",
       " ('dartBut', 1),\n",
       " ('Singapore', 1),\n",
       " ('eucalyptus', 1),\n",
       " ('Palebilled', 1),\n",
       " ('Flippy', 1),\n",
       " ('speciallyshaped', 1),\n",
       " ('dancelike', 1),\n",
       " ('sweetlips', 1),\n",
       " ('‘flying’', 1),\n",
       " ('yellowback', 1),\n",
       " ('parrotfish', 1),\n",
       " ('nudibranch', 1),\n",
       " ('moray', 1),\n",
       " ('parrotlike', 1),\n",
       " ('stinging', 1),\n",
       " ('Corals', 1),\n",
       " ('lettering', 1),\n",
       " ('Cleaner', 1),\n",
       " ('urchin', 1),\n",
       " ('oriental', 1),\n",
       " ('winglike', 1),\n",
       " ('Dugongs', 1),\n",
       " ('pipefish', 1),\n",
       " ('fusilier', 1),\n",
       " ('clownfish', 1),\n",
       " ('Pipefish', 1),\n",
       " ('buries', 1),\n",
       " ('jagged', 1),\n",
       " ('Parrotfish', 1),\n",
       " ('whitetip', 1),\n",
       " ('wrasse', 1),\n",
       " ('Feather', 1),\n",
       " ('tang', 1),\n",
       " ('grouper', 1),\n",
       " ('coral:', 1),\n",
       " ('luring', 1),\n",
       " ('batfish', 1),\n",
       " ('scrape', 1),\n",
       " ('Clownfish', 1),\n",
       " ('trumpetfish', 1),\n",
       " ('beautifullypatterned', 1),\n",
       " ('dugong', 1),\n",
       " ('dorsal', 1),\n",
       " ('sighted', 1),\n",
       " ('remora', 1),\n",
       " ('featherlike', 1),\n",
       " ('triggerfish', 1),\n",
       " ('Plankton', 1),\n",
       " ('‘arms’', 1),\n",
       " ('lionfish', 1),\n",
       " ('reaching', 1),\n",
       " ('Aiman’s', 1),\n",
       " ('Aiman', 1),\n",
       " ('Oof', 1),\n",
       " ('Density', 1),\n",
       " ('Sonam’s', 1),\n",
       " ('upward', 1),\n",
       " ('“Water', 1),\n",
       " ('buoyancy', 1),\n",
       " ('opposes', 1),\n",
       " ('spilled', 1),\n",
       " ('Lobsang', 1),\n",
       " ('delek', 1),\n",
       " ('“Or', 1),\n",
       " ('Genla:', 1),\n",
       " ('Sonam', 1),\n",
       " ('basketball', 1),\n",
       " ('displace', 1),\n",
       " ('apple’s', 1),\n",
       " ('“dip', 1),\n",
       " ('‘good', 1),\n",
       " ('naked', 1),\n",
       " ('Tashi', 1),\n",
       " ('fortune’', 1),\n",
       " ('”Tenzin', 1),\n",
       " ('greeting;', 1),\n",
       " ('“Tenzin', 1),\n",
       " ('”Genla', 1),\n",
       " ('displaced', 1),\n",
       " ('delek:', 1),\n",
       " ('Certainly', 1),\n",
       " ('Buoyancy', 1),\n",
       " ('TibetanGenla', 1),\n",
       " ('being’', 1),\n",
       " ('Gen', 1),\n",
       " ('Genla', 1),\n",
       " ('displaces', 1),\n",
       " ('tashi', 1),\n",
       " ('Tashi’s', 1),\n",
       " ('“Tashi', 1),\n",
       " ('Tenzin', 1),\n",
       " ('agreement', 1),\n",
       " ('“Again', 1),\n",
       " ('Displacement', 1),\n",
       " ('Rivulets', 1),\n",
       " ('Archimedes', 1),\n",
       " ('“Genla', 1),\n",
       " ('‘well', 1),\n",
       " ('CRAANKK', 1),\n",
       " ('Axle', 1),\n",
       " ('Wheel', 1),\n",
       " ('Lever', 1),\n",
       " ('Ammachi', 1),\n",
       " ('MACHINES', 1),\n",
       " ('PLEEEEEAASE', 1),\n",
       " ('YIPPEE', 1),\n",
       " ('machines:', 1),\n",
       " ('Screw', 1),\n",
       " ('Pulley', 1),\n",
       " ('YAY', 1),\n",
       " ('Sooraj', 1),\n",
       " ('WHUMP', 1),\n",
       " ('tray…', 1),\n",
       " ('shell…', 1),\n",
       " ('SQUEAK', 1),\n",
       " ('GRRRRRR', 1),\n",
       " ('ingredients…', 1),\n",
       " ('baba', 1),\n",
       " ('Toss', 1),\n",
       " ('slices…', 1),\n",
       " ('went:', 1),\n",
       " ('Wedge', 1),\n",
       " ('YUM', 1),\n",
       " ('SIMPLE', 1),\n",
       " ('grrrrrraaaaate', 1),\n",
       " ('crACK', 1),\n",
       " ('POTCH', 1),\n",
       " ('Sooraj’s', 1),\n",
       " ('KHATAK', 1),\n",
       " ('Inclined', 1),\n",
       " ('frrrrrrrruuuit', 1),\n",
       " ('Apple', 1),\n",
       " ('fruit)', 1),\n",
       " ('“Add', 1),\n",
       " ('sundried', 1),\n",
       " ('Potato', 1),\n",
       " ('“Baba', 1),\n",
       " ('Thamma’s', 1),\n",
       " ('Lemon', 1),\n",
       " ('jamTomato', 1),\n",
       " ('Thamma', 1),\n",
       " ('eaten)', 1),\n",
       " ('Oil', 1),\n",
       " ('revolves', 1),\n",
       " ('Paints', 1),\n",
       " ('Shrugs', 1),\n",
       " ('undersea', 1),\n",
       " ('stained', 1),\n",
       " ('(sunlight)', 1),\n",
       " ('sphere', 1),\n",
       " ('shushes', 1),\n",
       " ('‘spin’', 1),\n",
       " ('earth)', 1),\n",
       " ('starblanket', 1),\n",
       " ('axis', 1),\n",
       " ('Covers', 1),\n",
       " ('soundlessly', 1),\n",
       " ('frolic', 1),\n",
       " ('Eats', 1),\n",
       " ('rotation', 1),\n",
       " ('mermaid', 1),\n",
       " ('‘earth’', 1),\n",
       " ('Fusses', 1),\n",
       " ('sun)', 1),\n",
       " ('nightdark', 1),\n",
       " ('‘sunlight’', 1),\n",
       " ('Untangle', 1),\n",
       " ('itchy', 1),\n",
       " ('Rhino’s', 1),\n",
       " ('locodile', 1),\n",
       " ('Biger', 1),\n",
       " ('Ranga', 1),\n",
       " ('Change', 1),\n",
       " ('rhiger', 1),\n",
       " ('Simmy', 1),\n",
       " ('Bubbloo', 1),\n",
       " ('Tino', 1),\n",
       " ('smoother', 1),\n",
       " ('biger', 1),\n",
       " ('”Bubbloo', 1),\n",
       " ('”Tingu', 1),\n",
       " ('“Ranga', 1),\n",
       " ('rhear', 1),\n",
       " ('”Rani', 1),\n",
       " ('tino', 1),\n",
       " ('”“Ranga', 1),\n",
       " ('Rhear', 1),\n",
       " ('crion', 1),\n",
       " ('Rhiger', 1),\n",
       " ('bino', 1),\n",
       " ('EVERYTHING', 1),\n",
       " ('Secretly', 1),\n",
       " ('Cheekoo', 1),\n",
       " ('Cheekoo’s', 1),\n",
       " ('YUCK', 1),\n",
       " ('unhappiest', 1),\n",
       " ('littering', 1),\n",
       " ('TRASH', 1),\n",
       " ('Sweety', 1),\n",
       " ('Asha', 1),\n",
       " ('UGH', 1),\n",
       " ('COMPLETELY', 1),\n",
       " ('Bala', 1),\n",
       " ('Reema', 1),\n",
       " ('pagoda', 1),\n",
       " ('“Mark', 1),\n",
       " ('DHUPP', 1),\n",
       " ('Lo', 1),\n",
       " ('Smiling', 1),\n",
       " ('Myanmar', 1),\n",
       " ('tippy', 1),\n",
       " ('Anawrahta', 1),\n",
       " ('Langur', 1),\n",
       " ('clicketyclacks', 1),\n",
       " ('pistol', 1),\n",
       " ('Snap’s', 1),\n",
       " ('Storks', 1),\n",
       " ('Spotted', 1),\n",
       " ('Goby', 1),\n",
       " ('StorkMeet', 1),\n",
       " ('Hermit', 1),\n",
       " ('goby', 1),\n",
       " ('Snap', 1),\n",
       " ('CLACKET', 1),\n",
       " ('clawed', 1),\n",
       " ('Pairs', 1),\n",
       " ('CLICKETY', 1),\n",
       " ('Cattle', 1),\n",
       " ('Rumi’s', 1),\n",
       " ('Anu’s', 1),\n",
       " ('verbal', 1),\n",
       " ('”JHAL', 1),\n",
       " ('commentator', 1),\n",
       " ('Stumped', 1),\n",
       " ('edge…', 1),\n",
       " ('interschool', 1),\n",
       " ('TWAAANG', 1),\n",
       " ('“PLAY', 1),\n",
       " ('Umpires', 1),\n",
       " ('‘READY', 1),\n",
       " ('“READY', 1),\n",
       " ('aim', 1),\n",
       " ('Mithali', 1),\n",
       " ('Shanta', 1),\n",
       " ('RUNS', 1),\n",
       " ('Sohini', 1),\n",
       " ('hattrick', 1),\n",
       " ('bowling', 1),\n",
       " ('batswoman', 1),\n",
       " ('“Outside', 1),\n",
       " ('stump', 1),\n",
       " ('underarm', 1),\n",
       " ('declares', 1),\n",
       " ('“Yay', 1),\n",
       " ('‘PLAY’', 1),\n",
       " ('JHAL', 1),\n",
       " ('women’s', 1),\n",
       " ('Rumi', 1),\n",
       " ('Blind', 1),\n",
       " ('prompt', 1),\n",
       " ('“Another', 1),\n",
       " ('Hattrick', 1),\n",
       " ('Paula', 1),\n",
       " ('“Hattrick', 1),\n",
       " ('releasing', 1),\n",
       " ('Amita', 1),\n",
       " ('bowler', 1),\n",
       " ('‘YES’', 1),\n",
       " ('“Tails', 1),\n",
       " ('Howzzat', 1),\n",
       " ('visuallyimpaired', 1),\n",
       " ('crease', 1),\n",
       " ('Mridula', 1),\n",
       " ('Raj', 1),\n",
       " ('cue', 1),\n",
       " ('umpire', 1),\n",
       " ('TWHAAAACK', 1),\n",
       " ('theirs', 1),\n",
       " ('miaaww', 1),\n",
       " ('MOlecules', 1),\n",
       " ('rainbow:', 1),\n",
       " ('wavelengths)', 1),\n",
       " ('cloudstones', 1),\n",
       " ('ENORMOUS', 1),\n",
       " ('electromagnetic', 1),\n",
       " ('‘colour’', 1),\n",
       " ('cloudboats', 1),\n",
       " ('Spreads', 1),\n",
       " ('Washes', 1),\n",
       " ('of)', 1),\n",
       " ('Shatters', 1),\n",
       " ('BOUNCE', 1),\n",
       " ('rainbowcoloured', 1),\n",
       " ('RAINBOW', 1),\n",
       " ('from;', 1),\n",
       " ('cloudcottonwool', 1),\n",
       " ('Molecules', 1),\n",
       " ('Mostly', 1),\n",
       " ('Hardly', 1),\n",
       " ('violetindigobluegreenyelloworangered', 1),\n",
       " ('OVER', 1),\n",
       " ('wavelength)', 1),\n",
       " ('Shine', 1),\n",
       " ('longago', 1),\n",
       " ('Atmosphere', 1),\n",
       " ('interacts', 1),\n",
       " ('Millions', 1),\n",
       " ('WHEN', 1),\n",
       " ('Badaboom', 1),\n",
       " ('cloudy', 1),\n",
       " ('“WILL', 1),\n",
       " ('raincoat', 1),\n",
       " ('Bean', 1),\n",
       " ('Blindfold', 1),\n",
       " ('Swoof', 1),\n",
       " ('chickpea', 1),\n",
       " ('chick’s', 1),\n",
       " ('mung', 1),\n",
       " ('Sprouted', 1),\n",
       " ('soy', 1),\n",
       " ('nonMuslims', 1),\n",
       " ('seaworthy', 1),\n",
       " ('couch', 1),\n",
       " ('Dog’s', 1),\n",
       " ('spotty', 1),\n",
       " ('Ya', 1),\n",
       " ('Hairy', 1),\n",
       " ('tongueWhat', 1),\n",
       " ('tongueHeavy', 1),\n",
       " ('Giraffe’s', 1),\n",
       " ('Polar', 1),\n",
       " ('anthill', 1),\n",
       " ('krill', 1),\n",
       " ('Salamander', 1),\n",
       " ('Strong', 1),\n",
       " ('absorbs', 1),\n",
       " ('tongueLong', 1),\n",
       " ('sunburn', 1),\n",
       " ('Chameleon’s', 1),\n",
       " ('Cat’s', 1),\n",
       " ('Flamingo’s', 1),\n",
       " ('snatch', 1),\n",
       " ('stretchy', 1),\n",
       " ('savannah', 1),\n",
       " ('Pangolin', 1),\n",
       " ('heaviest', 1),\n",
       " ('Sticky', 1),\n",
       " ('tongueForked', 1),\n",
       " ('Whale', 1),\n",
       " ('tongueSpiky', 1),\n",
       " ('tongueRough', 1),\n",
       " ('lens;', 1),\n",
       " ('Mamu', 1),\n",
       " ('Motu', 1),\n",
       " ('“Saturn', 1),\n",
       " ('Chanda’s', 1),\n",
       " ('”“All', 1),\n",
       " ('Sharma', 1),\n",
       " ('starburst', 1),\n",
       " ('“Micro', 1),\n",
       " ('Motu’s', 1),\n",
       " ('planetarium', 1),\n",
       " ('Standing', 1),\n",
       " ('seethrough', 1),\n",
       " ('Binoculars', 1),\n",
       " ('Magnifying', 1),\n",
       " ('“Motu', 1),\n",
       " ('pathologist', 1),\n",
       " ('golis', 1),\n",
       " ('stargazing', 1),\n",
       " ('“Ooooh', 1),\n",
       " ('around—huge', 1),\n",
       " ('”Tinku', 1),\n",
       " ('Sloooowly', 1),\n",
       " ('“Didi', 1),\n",
       " ('enrich', 1),\n",
       " ('mbambamba', 1),\n",
       " ('mba', 1),\n",
       " ('mbamba', 1),\n",
       " ('tindiri', 1),\n",
       " ('Tindiri', 1),\n",
       " ('sizes:', 1),\n",
       " ('Sundays', 1),\n",
       " ('bul', 1),\n",
       " ('naming', 1),\n",
       " ('beater', 1),\n",
       " ('ti', 1),\n",
       " ('Kee', 1),\n",
       " ('sooooo', 1),\n",
       " ('Leaves', 1),\n",
       " ('swooped', 1),\n",
       " ('“Kee', 1),\n",
       " ('stickyFilm', 1),\n",
       " ('TikTik’s', 1),\n",
       " ('“TikTik', 1),\n",
       " ('Tape', 1),\n",
       " ('rip', 1),\n",
       " ('“Teacher', 1),\n",
       " ('”DO', 1),\n",
       " ('treeNow', 1),\n",
       " ('wallPlaster', 1),\n",
       " ('lizard’s', 1),\n",
       " ('TikTik', 1),\n",
       " ('offShoes', 1),\n",
       " ('THINK', 1),\n",
       " ('”“Akash', 1),\n",
       " ('“Akash', 1),\n",
       " ('Akash', 1),\n",
       " ('gummy', 1),\n",
       " ('hhhh', 1),\n",
       " ('hahahahaha', 1),\n",
       " ('clunkDon’t', 1),\n",
       " ('uhh', 1),\n",
       " ('clank', 1),\n",
       " ('Kit', 1),\n",
       " ('GARDENSDedicated', 1),\n",
       " ('WELCOME', 1),\n",
       " ('GARDENS', 1),\n",
       " ('ee', 1),\n",
       " ('Lauren', 1),\n",
       " ('“Each', 1),\n",
       " ('Mithai', 1),\n",
       " ('Payasam', 1),\n",
       " ('Trichy', 1),\n",
       " ('pooja', 1),\n",
       " ('Arabic', 1),\n",
       " ('Kela', 1),\n",
       " ('Surely', 1),\n",
       " ('Rasayana', 1),\n",
       " ('Shivanna', 1),\n",
       " ('Farms', 1),\n",
       " ('Gur', 1),\n",
       " ('Quality', 1),\n",
       " ('Farmer', 1),\n",
       " ('”Sringeri', 1),\n",
       " ('Doddooru', 1),\n",
       " ('“High', 1),\n",
       " ('Rawa', 1),\n",
       " ('‘finger’', 1),\n",
       " ('Pipal', 1),\n",
       " ('dosai', 1),\n",
       " ('o’clock', 1),\n",
       " ('Roja', 1),\n",
       " ('tock', 1),\n",
       " ('capseller', 1),\n",
       " ('Tick', 1),\n",
       " ('Roja’s', 1),\n",
       " ('Selvi', 1),\n",
       " ('”“AMMMAAAA', 1),\n",
       " ('“Paati', 1),\n",
       " ('kannamma', 1),\n",
       " ('pappadam', 1),\n",
       " ('“Selvi', 1),\n",
       " ('physician', 1),\n",
       " ('unbearable', 1),\n",
       " ('extract', 1),\n",
       " ('clean…”', 1),\n",
       " ('peevishly', 1),\n",
       " ('Coooo', 1),\n",
       " ('”Bounthy', 1),\n",
       " ('Coooooo', 1),\n",
       " ('Bounthy’s', 1),\n",
       " ('coooo', 1),\n",
       " ('Bounthy', 1),\n",
       " ('“Coooo', 1),\n",
       " ('oz)', 1),\n",
       " ('desired', 1),\n",
       " ('Mixture', 1),\n",
       " ('pleat', 1),\n",
       " ('Festival', 1),\n",
       " ('Luke', 1),\n",
       " ('(colors', 1),\n",
       " ('ladoo', 1),\n",
       " ('luke', 1),\n",
       " ('Evil', 1),\n",
       " ('November', 1),\n",
       " ('goody', 1),\n",
       " ('choice)', 1),\n",
       " ('microwave', 1),\n",
       " ('sparkler', 1),\n",
       " ('condensed', 1),\n",
       " ('chakli', 1),\n",
       " ('Peda', 1),\n",
       " ('\\u200bThe', 1),\n",
       " ('Torans', 1),\n",
       " ('Saffron', 1),\n",
       " ('Soak', 1),\n",
       " ('Goodness', 1),\n",
       " ('clarified', 1),\n",
       " ('Ingredients:', 1),\n",
       " ('cooperation', 1),\n",
       " ('Non', 1),\n",
       " ('titbit', 1),\n",
       " ('Yuri', 1),\n",
       " ('beef', 1),\n",
       " ('stomach–and', 1),\n",
       " ('“Chikki', 1),\n",
       " ('Gagarin’s', 1),\n",
       " ('“Scientists', 1),\n",
       " ('“even', 1),\n",
       " ('pipe–between', 1),\n",
       " ('flatbread', 1),\n",
       " ('wolfed', 1),\n",
       " ('”Chikki', 1),\n",
       " ('raita', 1),\n",
       " ('pouch', 1),\n",
       " ('vyomanauts', 1),\n",
       " ('Velcro', 1),\n",
       " ('canned', 1),\n",
       " ('spaceship’s', 1),\n",
       " ('topping', 1),\n",
       " ('“Nomnomnom', 1),\n",
       " ('freezer', 1),\n",
       " ('John', 1),\n",
       " ('cosmonaut', 1),\n",
       " ('“Toothpaste', 1),\n",
       " ('Glenn', 1),\n",
       " ('gobi', 1),\n",
       " ('piped', 1),\n",
       " ('sauce', 1),\n",
       " ('Ninu', 1),\n",
       " ('Cosmonaut', 1),\n",
       " ('fasten', 1),\n",
       " ('panipuri', 1),\n",
       " ('“Far', 1),\n",
       " ('tray', 1),\n",
       " ('liver', 1),\n",
       " ('“Mooli', 1),\n",
       " ('reheated', 1),\n",
       " ('pesky', 1),\n",
       " ('cabin', 1),\n",
       " ('granola', 1),\n",
       " ('suckling', 1),\n",
       " ('heredity', 1),\n",
       " ('rubberbands', 1),\n",
       " ('Laughing', 1),\n",
       " ('“Imma', 1),\n",
       " ('IMAGINARY', 1),\n",
       " ('Pupu', 1),\n",
       " ('humour', 1),\n",
       " ('“Wasn’t', 1),\n",
       " ('dishMake', 1),\n",
       " ('Influence:', 1),\n",
       " ('Ebok:', 1),\n",
       " ('ManipuriAppa', 1),\n",
       " ('*Ebok', 1),\n",
       " ('“Women', 1),\n",
       " ('Pupu:', 1),\n",
       " ('”Appa', 1),\n",
       " ('magically', 1),\n",
       " ('napkin', 1),\n",
       " ('(pg', 1),\n",
       " ('Imma', 1),\n",
       " ('Heredity:', 1),\n",
       " ('LOOK', 1),\n",
       " ('widest', 1),\n",
       " ('PappaCat', 1),\n",
       " ('Divided', 1),\n",
       " ('dimple', 1),\n",
       " ('MammaCat', 1),\n",
       " ('*Ebok:', 1),\n",
       " ('mewling', 1),\n",
       " ('Vadas:', 1),\n",
       " ('MammaCat’s', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(wordFreq.items(), key = lambda item: item[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[389, 148, 93, 10]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am being shy\"\n",
    "[wordFreq[lemmatizer.lemmatize(j)] for j in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDf['wordFreqCorpus'] = dataDf.Text.apply(lambda x: [wordFreq[lemmatizer.lemmatize(j)] for j in re.split(r\"\\.|\\!|\\?|\\ |\\'|\\\\|\\\"|\\,\", x) if j.lower() not in stop_words and len(j) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming words occuring less than 10 number of times as the less familiar words\n",
    "\n",
    "dataDf['familiarWords'] = dataDf.wordFreqCorpus.apply(lambda x: sum(np.array(x) >= 10))\n",
    "dataDf['nonFamiliarWords'] = dataDf.wordFreqCorpus.apply(lambda x: sum(np.array(x) < 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syllable Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syllable count helps us in deciding if the word is difficult to understand\n",
    "\n",
    "def syllable_count(word):\n",
    "    if(len(word) == 0):\n",
    "        return 0\n",
    "    count = 0\n",
    "    vowels = \"aeiouyAEIOUY\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf['syllableCount'] = dataDf.Text.apply(lambda x: [syllable_count(j) for j in re.split(\"!|\\.|\\?|\\ \", x) if j.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDf['maxSyllableCount'] = dataDf.syllableCount.apply(lambda x: max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf['avgSyllableCount'] = dataDf.syllableCount.apply(lambda x: mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of syllables will also help in deciding complexity of the text\n",
    "dataDf['smallSyllableCount'] = dataDf.syllableCount.apply(lambda x: sum(np.array(x) < 3))  \n",
    "dataDf['largeSyllableCount'] = dataDf.syllableCount.apply(lambda x: sum(np.array(x) >= 3))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'fileName', 'StoryLength', 'wordLength', 'maxWordLength',\n",
       "       'avgWordLength', 'smallWords', 'largeWords', 'sentenceLengthChar',\n",
       "       'maxSentenceLengthChar', 'avgSentenceLengthChar', 'smallSentenceChar',\n",
       "       'largeSentenceChar', 'sentenceLengthWord', 'maxSentenceLengthWord',\n",
       "       'avgSentenceLengthWord', 'smallSentenceWord', 'largeSentenceWord',\n",
       "       'wordFreqCorpus', 'familiarWords', 'nonFamiliarWords', 'syllableCount',\n",
       "       'maxSyllableCount', 'avgSyllableCount', 'smallSyllableCount',\n",
       "       'largeSyllableCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDf.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf.to_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf[['Text', 'fileName', 'StoryLength', 'maxWordLength', 'avgWordLength','smallWords', 'largeWords',\n",
    "       'maxSentenceLengthChar', 'avgSentenceLengthChar', 'smallSentenceChar',\n",
    "       'largeSentenceChar', 'maxSentenceLengthWord','avgSentenceLengthWord', \n",
    "       'smallSentenceWord', 'largeSentenceWord','familiarWords', 'nonFamiliarWords',\n",
    "       'maxSyllableCount', 'avgSyllableCount', 'smallSyllableCount',\n",
    "       'largeSyllableCount']].to_csv('processedOutput.tsv', sep = '\\t',index = False, line_terminator='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
