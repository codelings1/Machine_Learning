{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data from Big Query to Cloud Sql via Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Storage Bucket <br> \n",
    "1. Open <strong>[Storage Bucket](https://console.cloud.google.com/storage)</strong><br>\n",
    "2. Specify your \"Bucket_Name\" <br>\n",
    "3. Set the location: \"Multiregional US\" <br>\n",
    "4. <strong>[Bucket.img](bucket.png)</strong><br>\n",
    "5. Create a<strong> [folder](bucket2.png)</strong> in your bucket with 2 subfolders named staging and temp\n",
    "6. Go to overview to get your bucket directory <strong>[link](bucket3.png).</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Biq Query Database <br>\n",
    "    \n",
    "1. Open <strong>[Big Query](https://console.cloud.google.com/bigquery)</strong> and Create a dataset\n",
    "2. Create Table using a local csv <strong>[dataset](test.txt)</strong> file.(Save file as .csv)<br>\n",
    "3. For schema use<strong> [Auto Detect](bquery.png) </strong>option and Click create.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud Sql Instace\n",
    "1. Create a new Instance (MySql/Postgres).<br>\n",
    "2. Set the info of sql instance and continue.<br>\n",
    "3. Under <strong>[Instance Section](sql.png)</strong> go to Database and creat a new database.<br>\n",
    "4. Open Cloud Shell Machine to create a new table in database.<br>\n",
    "5. Use command<br> \"gcloud sql connect [INSTANCE_ID] --user=root\" <br>to start mysql on shell.<br><br>\n",
    "6. Create a new table in the database you created and set the schema as per needs. Here we made the table with Name and Sex attributes.<br><br>\n",
    "References:\n",
    "<strong>[Link 1](https://cloud.google.com/sql/docs/mysql/connect-admin-ip) <br></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> In order to write a Java Dataflow job, you will first need to download the SDK from the Maven repository.<br><br>\n",
    "\n",
    "When you run this command, Maven will create a project structure and config file for downloading the appropriate version of the Apache Beam SDK.<br> </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open your cloud shell<br>\n",
    "2. Go to a directory where you want to create your Maven Repo.<br>\n",
    "3. Run command: <br><br>\n",
    "    <strong>mvn archetype:generate -DarchetypeGroupId=org.apache.beam -DarchetypeArtifactId=beam-sdks-java-maven-archetypes-examples -DgroupId=com.example -DartifactId=Dataflow_intro -Dversion=\"0.1\" -DinteractiveMode=false -Dpackage=com.example<br></strong><br>\n",
    "4. Traverse to \"Dataflow_intro/src/main/java/com/example\" and save your custom <strong>[Template.java](BQuerySqlFlow.java) file.</strong><br><br>\n",
    "<h6>\n",
    "Read the template.java to edit some stuff out.\n",
    "</h6>\n",
    "<br><br>\n",
    "5. return to Dataflow_intro, check for pom.xml file, this will have all the dependencies that we need for our job to run.\n",
    "6. run the command to create a dataflow job:<br><br>\n",
    "<strong>mvn compile exec:java -e -Dexec.mainClass=com.click.example.BQuerySqlflow -Dexec.args=\" --project=dark-pipe-247510 --stagingLocation=gs://datasets14/staging/ --tempLocation=gs://datasets14/temp/ --templateLocation=gs://dark-pipe-247510/template1.json --runner=DataflowRunner\"\n",
    "</strong><br><br>\n",
    "~Change project id, staging, temp, template location to your specifications. We use template location to save our template as json file in bucket to use it for scheduling purposes.<br><br>\n",
    "\n",
    "After your <strong>[Build](dataflow_job.png)</strong> is successful<br><br>\n",
    "\n",
    "Open Dataflow <strong>[Jobs](https://console.cloud.google.com/dataflow)</strong> Select your job to view the description<br>\n",
    "<strong>[PipelineJob](dataflow_job2.png)</strong>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Cloud Sql to see the [results](sql2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~thanks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
